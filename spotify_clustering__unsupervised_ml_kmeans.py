# -*- coding: utf-8 -*-
"""Spotify_clustering_ Unsupervised ML_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15XWArPtjZl7SGJIYqStCSlQmaT6OGsMY
"""

import pandas as pd
import numpy as np
from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from sklearn import set_config
set_config(transform_output="pandas")

url = "https://drive.google.com/file/d/1CKABqo8Gj9f4KuVxjnKPTlMu009Y7jxG/view?usp=sharing"
path = "https://drive.google.com/uc?export=download&id="+url.split("/")[-2]
spotify_df = pd.read_csv(path)

#check the data
spotify_df.columns=spotify_df.columns.str.strip()

#spotify_df['time_signature'].unique()
spotify_df['song'] = spotify_df['artist'] +' *** '+ spotify_df['name']

#spotify_df['song']

spotify_df['id'].duplicated().sum()

spotify_df.drop_duplicates('song',inplace=True)

#spotify_df.duplicated().sum()
#spotify_df.info()
#spotify_df.iloc[:, -5]


#cleaning the data frame

spotify_df=spotify_df.set_index('song')

#'type' was empty
col_to_del=[ 'Unnamed: 0', 'artist', 'name', 'key','mode', 'type', 'time_signature','id', 'html']
spotify_df = spotify_df.drop(columns=col_to_del, axis=1)

spotify_df.columns

#checking what columns I need to scale/transform
#loudness
#tempo
#duration_ms
spotify_df.describe()

"""## What needs to be done:

- check with different scaler/ transformer what is the best to use
- PCA
- check with silouhette and inertia how many clusters should be there (rememeber how many was advised by business)
- perform Kmean analysis and label all the tracks
-check the tracks (listen / perfrmorm radar chart(Not.3))
"""

#scale only columns: loudness, tempo, duration_ms

# Create a MinMaxScaler object
scaler = MinMaxScaler().set_output(transform="pandas")

# Scale the foods_df DataFrame
scaled_spotify=spotify_df.copy()
scaled_spotify[['loudness', 'tempo', 'duration_ms']] = scaler.fit_transform(scaled_spotify[['loudness', 'tempo', 'duration_ms']])

#spotify_df.describe()
scaled_spotify.describe()

"""PCA  Principal componenet analysis"""

# Initialise the PCA object
pca = PCA()

# Fit the PCA object to the data
pca.fit(scaled_spotify)

# Transform scaled_features_df based on the fit calculations
pca_basic_df = pca.transform(scaled_spotify)

pca_basic_df

# Get the variance explained by each principal component
explained_variance_array = pca.explained_variance_ratio_

explained_variance_array
#principal components are ranked in order of decreasing variance, with the first principal component explaining the most variance in the data

# I wiil use Cumulative explained variance to determine how many principal components contain a given amount of variance, such as 95%.

cumulative_sum_of_variance = np.cumsum(explained_variance_array)

cumulative_sum_of_variance

(
  # Create a cumulative explained variance plot
  sns.relplot(
      kind = "line",  # Create a line plot
      x = range(len(explained_variance_array)),  # Set the x-axis to be the principal component index
      y = cumulative_sum_of_variance,  # Set the y-axis to be the cumulative explained variance
      marker = "o",  # Use a circle marker for the data points
      aspect = 1.4,  # Set the aspect ratio of the plot to be 1.4
  )
  # Set the title of the plot
  .set(title = "Cumulative explained variance of principal components")
  # Set the axis labels
  .set_axis_labels("Principal component", "Cumulative explained variance")
);

# Create a PCA object
pca_variance = PCA(n_components = 0.95)

(
  # Create a cumulative explained variance plot
  sns.relplot(
      kind = "line",  # Create a line plot
      x = range(len(explained_variance_array)),  # Set the x-axis to be the principal component index
      y = cumulative_sum_of_variance,  # Set the y-axis to be the cumulative explained variance
      marker = "o",  # Use a circle marker for the data points
      aspect = 1.4,  # Set the aspect ratio of the plot to be 1.4
  )
  # Set the title of the plot
  .set(title="Cumulative explained variance of principal components")
  # Set the axis labels
  .set_axis_labels("Principal component", "Cumulative explained variance")
);

# Add a horizontal red line at 0.95 on the y axis
plt.axhline(y = 0.95,
            color = 'red');

# Fit the PCA object to the scaled features dataframe and transform it
pca_variance_df = pca_variance.fit_transform(scaled_spotify)

# The dataframe now contains the principal components of the scaled features dataframe
pca_variance_df

"""PCA gere was not really needed as the number of features is not big. So we have done PCA but not necessary need to analyse further with it. Do further I will work with scaled data

## K-numbers
We have here 2 possible way to find the perfect number of clusters: inertia and silouhette score.

**Inertia and elbow method:**
Once we've calculated the inertia scores for all values of k, we'll plot them on a line chart. We'll then look for the "elbow" in the line chart. The elbow is the point where the inertia score starts to plateau, indicating that adding more clusters is not significantly improving the quality of the clustering.
"""

#inertia
# Decide on a random_state to use
seed = 2

# Set the maximum number of clusters to try
max_k = 100

# Create an empty list to store the inertia scores
inertia_list = []

# Iterate over the range of cluster numbers
for i in range(1, max_k + 1):

    # Create a KMeans object with the specified number of clusters
    myKMeans = KMeans(n_clusters = i,
                      n_init = "auto",
                      random_state = seed)

    # Fit the KMeans model to the scaled data
    myKMeans.fit(scaled_spotify)

    # Append the inertia score to the list
    inertia_list.append(myKMeans.inertia_)

inertia_list

# Set Seaborn theme
sns.set_theme(style='darkgrid')

# Create the line plot
g = sns.relplot(y=inertia_list,
                x=range(1, max_k + 1),
                kind='line',
                marker='o',
                height=8,
                aspect=2)

# Set plot title and labels
g.set(title=f"Inertia score from 1 to {max_k} clusters",
      xlabel="Number of clusters",
      ylabel="Inertia score")

# Draw a vertical red line at x = 50
for ax in g.axes.flat:  # This ensures compatibility with FacetGrid
    ax.axvline(x=50, color='red', linestyle="--")

# Show the plot
plt.show()

#

"""The **silhouette score** is another metric for evaluating the quality of clustering results. It measures how well each data point is assigned to its cluster, taking into account the distance to other clusters. The silhouette score ranges from -1 to 1, with higher scores indicating better clustering."""

# Set the maximum number of clusters to try
max_k = 100

# Create an empty list to store the silhouette scores
sil_scores = []


for j in range(2, max_k+1):

    # Create a KMeans object with the specified number of clusters
    kmeans = KMeans(n_clusters = j,
                    n_init = "auto",
                    random_state = seed)

    # Fit the KMeans model to the scaled data
    kmeans.fit(scaled_spotify)

    # Get the cluster labels
    labels = kmeans.labels_

    # Calculate the silhouette score
    score = silhouette_score(scaled_spotify, labels)

    # Append the silhouette score to the list
    sil_scores.append(score)

sil_scores

(
sns.relplot(y = sil_scores,
            x = range(2, max_k+1),
            kind = 'line',
            marker = 'o',
            height = 8,
            aspect = 2)
.set(title=f"Silhouette score from 2 to {max_k } clusters")
.set_axis_labels("Number of clusters", "Silhouette score")
);

"""To determine the optimal number of clusters, we can look for the highest silhouette score. This score measures how well each point is assigned to its cluster, with higher scores indicating better clustering. So for us it would be smth between 2 and 5.

However, the highest silhouette score may not always be the best choice. For example, if the score is very high for a large number of clusters, it may be a sign that the clusters are too finely grained and that we are overfitting the data.

In addition, we should consider our business objectives and the data we are using. And that's teh reason we will choose inertia method, because it respond to our business needs, 2-5 playlist, would be too small number for us (we need to choose between 20-100).

**Using KMeans do divide songs into 50 clusters**
"""

# Initialise the model
my_kmeans = KMeans(n_clusters = 50, # you always choose the number of k here
                   random_state = 2)

# Fit the model to the data
my_kmeans.fit(scaled_spotify)

# Obtain the cluster output
playlist = my_kmeans.labels_

# Attach the cluster output to our original DataFrame
scaled_spotify["playlist"] = playlist

scaled_spotify

scaled_spotify.groupby(by="playlist").mean()

"""## Radar chart"""

# Create an empty list to store the Scatterpolar object of each cluster
scatter_objects = []

# State the label for each arm of the chart
categories = scaled_spotify.columns

# Iterate over the unique clusters and add an object for each cluster to the list
for playlist in sorted(scaled_spotify['playlist'].unique()):

  # Find the mean value for each column of the cluster
  cluster_means = [scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[0]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[1]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[2]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[3]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[4]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[5]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[6]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[7]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[8]].mean(),
                   scaled_spotify.loc[scaled_spotify["playlist"] == playlist, scaled_spotify.columns[9]].mean()]

  cluster_scatter = go.Scatterpolar(
    r = cluster_means, # set the radial coordinates
    theta = categories, # the names of the columns
    fill = 'toself', # fills in the space with colour
    name = f'Playlist {playlist}' # adds the name of the cluster
  )

  # Add the Scatterpolar object to the list
  scatter_objects.append(cluster_scatter)

# Create the figure (the white area)
fig = go.Figure()

# Add the scatter objects to the figure
fig.add_traces(scatter_objects)

# Add extras to the plot, such as title
fig.update_layout(
  title_text = 'Radar chart of mean features of songs in the playlist',
  height = 600,
  width = 800,
  polar = dict(
    radialaxis = dict(
      visible = True, # visibility of the numbers on the arm
      range = [0, 1] # scale of the plot
    )),
  showlegend = True
)

# Show the initialised plot and the trace objects
fig.show()

#check live
#scaled_spotify.sort_values('playlist')
scaled_spotify.loc[scaled_spotify['playlist']==16, :].head(50)

"""So far we have our songs devided into playlist and if needed we can work further with Spotify API to create this songlists straight into our account.

# Getting Started with Spotify

1. First, **navigate to [Spotify's Developer Website](https://developer.spotify.com/.),** register, and click the Create app button. Give your app a name and brief description, and enter a Redirect URI.

2. For Google Colab we set the RedirectURL to http://localhost:8081/

Otherwise, you may need to try multiple redirect URIs before finding one that works. Other options include localhost:8000 and localhost:8888.

The important thing is that the host you specify in the Spotify dashboard matches the host in the code below.

After agreeing to the terms and conditions, you can move on.

3. **Getting the Client ID and Client Secret**.
rom the dashboard of your new app, navigate to Settings and copy the Client ID and Client Secret.

4. Finally, we navigate to **[Spotify Web]( https://open.spotify.com/.)** (not the developer site), and click the profile icon in the upper right, then "Account", follwed by "Edit profile" to get our Username. This may be our display name, such as wbsLovesMusic, or a random hash of letters and numbers.

With all this information saved, we're ready to post our first playlist.
"""

# !pip install spotipy
# OR
# !conda install -c conda-forge spotipy

import spotipy
import pickle
from spotipy.oauth2 import SpotifyOAuth
from google.colab import userdata # only in Google Colab for secret management

scope = 'playlist-modify-public'
username = 'YOUR_USERNAME'
redirectUri = 'URI_MUST_MATCH_THAT_SET_IN_APP_CREATION'
client_id = 'YOUR_APPS_CLIENT_ID'
client_secret = 'YOUR_APP_CLIENT_SECRET'

token = SpotifyOAuth(scope=scope,
                     username=username,
                     client_id=client_id,
                     client_secret=client_secret,
                     redirect_uri=redirectUri,
                     open_browser=False # this line is need in Colab, but not on local machine
                     )
spotifyObject = spotipy.Spotify(auth_manager = token)

playlist_collection = {}
for i in range(n_clusters):  # This range must match the number of clusters
    playlist_name = f'my_playlist_cluster_{i}'  # Here, we will create simple playlist names indicating which cluster created the playlist
    playlist_description= 'this is a test list'
    playlist_id = spotifyObject.user_playlist_create(user=username,
                                                     name=playlist_name,
                                                     public=True,
                                                     description=playlist_description)['id']
    id_list = list(songs_df.loc[songs_df["cluster"] == i]
                            .sample(5)  # Here, we simply take 5 songs randomly from each cluster
                            .id
                   )
     # Spotify needs cetain text wrappers around ID numbers to identify the target as a track, a playlist, etc.
    uris = [f'spotify:track:{str(id).strip()}' for id in id_list]
    spotifyObject.user_playlist_add_tracks(user=username,playlist_id=playlist_id,tracks=uris)
    playlist_collection[playlist_name] = playlist_id

# When this code is run, either a browser window will open, or in Google Colab a URL will appear
# This URL will ask you to sign in to Spotify
# After signing in, copy the URL you are redirected to and paste it in the box that pops up

# Playlist IDs can be saved for later
with open("./playlists.pkl", "wb") as f:
    pickle.dump(playlist_collection, f)

# This step is only needed in Google Colab
from google.colab import files
files.download("./playlists.pkl")